default_model: "gemini_flash"

retry_config:
  num_retries: 10  # Reduced from 20 to prevent excessive retry delays
  cooldown_seconds: 2
  max_backoff_seconds: 30  # Cap exponential backoff at 2 minutes

parsing_retry_config:
  num_retries: 5
  cooldown_seconds: 1.0

rate_limits:
  "gemini/gemini-2.5-flash-lite":
    requests_per_minute: 4000
    input_tokens_per_minute: 4000000
  "gemini/gemini-2.5-flash":
    requests_per_minute: 1000
    input_tokens_per_minute: 1000000
  "gemini/gemini-2.5-pro":
    requests_per_minute: 150
    input_tokens_per_minute: 2000000
  "gemini/gemini-3-pro-preview":
    requests_per_minute: 50
    input_tokens_per_minute: 1000000
  "openai/gpt-5-mini":
    requests_per_minute: 60
    input_tokens_per_minute: 1000000
  "deepseek/deepseek-chat":
    requests_per_minute: 60
    input_tokens_per_minute: 1000000

models:
  gemini_3_pro:
    provider: "gemini"
    model: "gemini/gemini-3-pro-preview"
    api_key: "${oc.env:GEMINI_API_KEY}"
    temperature: 1.0
    max_tokens: 200000
    timeout: 1200  # 20 minutes for long-context model
  gemini_pro:
    provider: "gemini"
    model: "gemini/gemini-2.5-pro"
    api_key: "${oc.env:GEMINI_API_KEY}"
    temperature: 0.1
    max_tokens: 200000
    thinking:
      type: "enabled"
      budget_tokens: 32768
    timeout: 1200  # 20 minutes for long-context model
  gemini_flash_lite:
    provider: "gemini"
    model: "gemini/gemini-2.5-flash-lite"
    api_key: "${oc.env:GEMINI_API_KEY}"
    temperature: 0.1
    max_tokens: 32768
    thinking:
      type: "enabled"
      budget_tokens: 24576
  gemini_flash:
    provider: "gemini"
    model: "gemini/gemini-2.5-flash"
    api_key: "${oc.env:GEMINI_API_KEY}"
    temperature: 0.1
    max_tokens: 150000
    thinking:
      type: "enabled"
      budget_tokens: 24576
  gpt5_mini:
    provider: "openai"
    model: "openai/gpt-5-mini"
    api_key: "${oc.env:OPENAI_API_KEY}"
    temperature: 1
    max_tokens: 32768
    reasoning_effort: "medium"
  gpt5:
    provider: "openai"
    model: "openai/gpt-5"
    api_key: "${oc.env:OPENAI_API_KEY}"
    temperature: 1
    max_tokens: 131072
    reasoning_effort: "high"
  gpt5_1:
    provider: "openai"
    model: "openai/gpt-5.1"
    api_key: "${oc.env:OPENAI_API_KEY}"
    temperature: 1
    max_tokens: 128000
    reasoning_effort: "high"
  gpt5_2:
    provider: "openai"
    model: "openai/gpt-5.2"
    api_key: "${oc.env:OPENAI_API_KEY}"
    temperature: 1
    max_tokens: 128000
    reasoning_effort: "high"
  deepseek_chat:
    provider: "deepseek"
    model: "deepseek/deepseek-chat"
    api_key: "${oc.env:DEEPSEEK_API_KEY}"
    temperature: 0.5
    max_tokens: 4096
  vllm_local:
    provider: "vllm"
    model: "Qwen/Qwen3-4B-Instruct-2507"
    api_key: "${oc.env:VLLM_API_KEY}"
    api_base: "${oc.env:VLLM_API_BASE}"
    temperature: 0.6
    max_tokens: 65536
    completion_mode: true
    thinking_block_token: "<think>"
    context_window: 65500
    extra_params:
      top_p: 0.95
      top_k: 20
  deepseek_math_v2:
    provider: "vllm"
    model: "deepseek/deepseek-math-v2"
    api_key: "${oc.env:VLLM_API_KEY}"
    api_base: "${oc.env:VLLM_API_BASE}"
    temperature: 0.3
    max_tokens: 32000
    completion_mode: false


 

